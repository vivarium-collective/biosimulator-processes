{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-21T19:58:40.075077Z",
     "start_time": "2024-05-21T19:58:38.555281Z"
    }
   },
   "source": [
    "import sys\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "from process_bigraph import pp\n",
    "\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "\n",
    "from biosimulator_processes.services.rest_service import BiosimulationsRestService"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:58:40.099182Z",
     "start_time": "2024-05-21T19:58:40.076169Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tellurium as te \n",
    "\n",
    "teModel = te.loadSBMLModel('/Users/alexanderpatrie/Desktop/repos/biosimulator-processes/test_suite/examples/sbml-core/Caravagna-J-Theor-Biol-2010-tumor-suppressive-oscillations/Caravagna2010.xml')\n",
    "\n",
    "\n",
    "results = teModel.simulate(0, 100.0, 1000)"
   ],
   "id": "51505516b1a7bbd6",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:58:40.153255Z",
     "start_time": "2024-05-21T19:58:40.099880Z"
    }
   },
   "cell_type": "code",
   "source": "teModel.plot()",
   "id": "7b7907231df4a571",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:58:42.939959Z",
     "start_time": "2024-05-21T19:58:40.153887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "repressilator_run_id = \"61fea4a08c1e3dc95a79802e\"\n",
    "\n",
    "query = \"elowitz\"\n",
    "\n",
    "\n",
    "# TODO: Refactor this to just be able to take in an OMEX archive. \n",
    "# TODO: Load expected results json from above.\n",
    "\n",
    "\n",
    "def download_simulation_report_file(url: str, save_dir=None) -> str:\n",
    "    from tempfile import mkdtemp\n",
    "    import requests \n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    local_filename = \"reports.h5\"\n",
    "    download_dir = save_dir or mkdtemp()\n",
    "    fp = os.path.join(download_dir, local_filename)\n",
    "    if response.status_code == 200:\n",
    "        with open(fp, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=8192): \n",
    "                if chunk: # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "        return fp \n",
    "    else:\n",
    "        print(f\"Failed to download file. Status code: {response.status_code}\")\n",
    "\n",
    "\n",
    "async def fetch_simulation_report_file(query: str):\n",
    "    from tempfile import mkdtemp\n",
    "    service = BiosimulationsRestService()\n",
    "    result = await service.fetch_files(query)\n",
    "    for f in result.files:\n",
    "        file_url = f['url']\n",
    "        if '.h5' in file_url:\n",
    "            save_dir = mkdtemp()\n",
    "            report_fp = download_simulation_report_file(file_url, save_dir)\n",
    "            return service.read_report_outputs(report_fp)\n",
    "            \n",
    "            \n",
    "await fetch_simulation_report_file(query)"
   ],
   "id": "792b41d7a7b6007",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:58:42.941445Z",
     "start_time": "2024-05-21T19:58:42.941395Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from basico import *\n",
    "\n",
    "model_fp = '/Users/alexanderpatrie/Desktop/repos/biosimulator-processes/demos/mtor-signaling.xml'\n",
    "\n",
    "basicoModel = load_model(model_fp)"
   ],
   "id": "d662a5d917e52a42",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "basicoModel",
   "id": "e4af8267e270eb8f",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "get_files_url = f'https://api.biosimulations.dev/results/{repressilator_run_id}/download'\n",
    "headers = {'accept': 'application/json'}\n",
    "files_resp = requests.get(get_files_url, headers=headers)"
   ],
   "id": "43e85b4f535dd796",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "files_resp.json()",
   "id": "a5c17e52eafdec21",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "\n",
    "def explore_hdf5(file_path):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        def print_attrs(name, obj):\n",
    "            print(f\"Name: {name}\")\n",
    "            print(\"Attributes:\")\n",
    "            for key, val in obj.attrs.items():\n",
    "                print(f\"    {key}: {val}\")\n",
    "            if isinstance(obj, h5py.Dataset):\n",
    "                print(f\"Dataset shape: {obj.shape}\")\n",
    "                print(f\"Dataset dtype: {obj.dtype}\")\n",
    "            elif isinstance(obj, h5py.Group):\n",
    "                print(\"Group\")\n",
    "            print()\n",
    "        \n",
    "        f.visititems(print_attrs)\n",
    "\n",
    "# Usage example\n",
    "file_path = '/Users/alexanderpatrie/Desktop/repos/biosimulator-processes/test_suite/examples/sbml-core/Elowitz-Nature-2000-Repressilator/reports.h5'\n",
    "explore_hdf5(file_path)"
   ],
   "id": "a32cf79c43b3a52d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def read_dataset(file_path, dataset_id):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        if dataset_id in f:\n",
    "            data = f[dataset_id][:]\n",
    "            print(f\"Data from dataset '{dataset_id}':\\n{data}\")\n",
    "        else:\n",
    "            print(f\"Dataset '{dataset_id}' not found in the file.\")\n",
    "            \n",
    "            \n",
    "read_dataset(file_path, 'data_set_time')"
   ],
   "id": "50e94bdbad1de41f",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from biosimulator_processes.services.rest_service import BiosimulationsRestService",
   "id": "e4079a96d9ec5db3",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "5a18bfb627415dae",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from basico import * \n",
    "\n",
    "\n",
    "fp = '/Users/alexanderpatrie/Desktop/repos/biosimulator-processes/demos/BIOMD0000000012_url.xml'\n",
    "\n",
    "model = load_model(fp)"
   ],
   "id": "cb63a39f24a3794f",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "get_species(model=model)",
   "id": "c4ce852efe9e1ca1",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from biosimulator_processes.processes.copasi_process import CopasiProcess\n",
    "group_path = 'simulation.sedml/report'\n",
    "dataset_label = 'cI protein'\n",
    "outputs = BiosimulationsRestService().read_report_outputs(file_path, dataset_label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "process = CopasiProcess(config={'model': {'model_source': fp}})"
   ],
   "id": "426f89565958f483",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "process.inputs()[f'floating_species_concentrations']",
   "id": "4e34292653057f9a",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from biosimulator_processes.verify.core import verify_ode_process_outputs\n",
    "\n",
    "\n",
    "verification = verify_ode_process_outputs('copasi', file_path, fp)"
   ],
   "id": "7cb779ab9700e3b9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "process.initial_state()['floating_species_concentrations'].keys()",
   "id": "fe264b58abab6899",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def is_equal(a, b):\n",
    "    return a == b or b == a  \n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from process_bigraph import Process\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class OutputAspectVerification:\n",
    "    aspect_type: str  # one of: 'names', 'values'. TODO: Add more\n",
    "    is_verified: bool\n",
    "    \n",
    "\n",
    "def create_ode_process_instance(process_name: str, biomodel_id=None, sbml_model_file=None) -> Process:\n",
    "    module_name = f'{process_name}_process'\n",
    "    import_statement = f'biosimulator_processes.processes.{module_name}'\n",
    "    module_paths = module_name.split('_')\n",
    "    class_name = module_paths[0].replace(module_name[0], module_name[0].upper())\n",
    "    class_name += module_paths[1].replace(module_paths[1][0], module_paths[1][0].upper())\n",
    "    module = __import__(\n",
    "        import_statement, fromlist=[class_name])\n",
    "    model_source = biomodel_id or sbml_model_file\n",
    "    bigraph_class = getattr(module, class_name)\n",
    "    return bigraph_class(config={'model': {'model_source':model_source}})\n",
    "\n",
    "\n",
    "def verify_ode_process_output_names(process_name: str, source_report_fp: str, biomodel_id: str = None, sbml_model_file: str = None) -> OutputAspectVerification:\n",
    "    # Get the class from the module\n",
    "    # TODO: Automatically generate this from the biosimulations rest server\n",
    "    process = create_ode_process_instance(process_name, biomodel_id, sbml_model_file)\n",
    "    process_keys = list(process.inputs()['floating_species_concentrations'].keys())\n",
    "    \n",
    "    report_outputs = BiosimulationsRestService().read_report_outputs(report_file_path=source_report_fp)\n",
    "    report_keys = [datum.dataset_label for datum in report_outputs.data]\n",
    "    for i, val in enumerate(report_keys):\n",
    "        if report_keys[i].lower() == 'time':\n",
    "            report_keys.pop(i)\n",
    "    \n",
    "    return OutputAspectVerification(aspect_type='names', is_verified=is_equal(report_keys, process_keys))"
   ],
   "id": "3802dbae678cdaaa",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from biosimulator_processes.verify.core import verify_ode_process_output_names\n",
    "\n",
    "\n",
    "verify_ode_process_output_names(process_name='copasi', source_report_fp=file_path, sbml_model_file=fp)"
   ],
   "id": "301900dd238ff637",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = np.array([1, 2, 3])\n",
    "y = np.array([1, 2, 3.0000001])"
   ],
   "id": "b5735ea91ffa5ada",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "np.allclose(x, y)",
   "id": "a18a4b13fc1307d9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class TokenizedWord:\n",
    "    def __init__(self, word: str, alphabet=None, language='english'):\n",
    "        super().__init__()\n",
    "        self.word = word\n",
    "        self.alphabet = alphabet or [chr(i) for i in range(ord('a'), ord('z') + 1)]\n",
    "        self.language = language\n",
    "        self.data = self.tokenize()\n",
    "        self.encoded = list(self.data.values())\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self.tokenize())\n",
    "    \n",
    "    def tokenized_word(self):\n",
    "        return list(self.tokenize().values())\n",
    "    \n",
    "    def _create_mapping(self, data):\n",
    "        return {float(i): letter for i, letter in enumerate(data)}\n",
    "    \n",
    "    def transform_data(self, data, r: tuple) -> np.ndarray:\n",
    "        \"\"\"Transform the `data` to fit range `r`, where `r=(rangeStart, rangeStop)`\"\"\"\n",
    "        min_orig = min(data)\n",
    "        max_orig = max(data)\n",
    "        normalized_data = [a + ((x - min_orig) * (r[1] - r[0]) / (max_orig - min_orig)) for x in data]\n",
    "        return np.array(normalized_data, dtype='float64')\n",
    "    \n",
    "    def normalize(self, data, a, z):\n",
    "        min_orig = min(data)\n",
    "        max_orig = max(data)\n",
    "        \n",
    "        # Apply min-max normalization\n",
    "        normalized_data = [a + ((x - min_orig) * (z - a) / (max_orig - min_orig)) for x in data]\n",
    "        return normalized_data\n",
    "    \n",
    "    def tokenize(self):\n",
    "        # 1. create alphabet mapping (implement this for any given spoken language/music. For example, 12 tones mapped to the harmonic series\n",
    "        alphabet_data = self.alphabet\n",
    "        alphabet_mapping = self._create_mapping(alphabet_data)\n",
    "        \n",
    "        # 2. transform letter mapping range to fit 0-9, as double digits cannot be verified/read, which in python is 0, 9+1\n",
    "        alphabet_index = [int(n) for n in list(alphabet_mapping.keys())]\n",
    "        \n",
    "        input_range = [int(min(list(alphabet_mapping.keys()))), int(max(list(alphabet_mapping.keys())))]\n",
    "        start = input_range[0]\n",
    "        end = input_range[1]\n",
    "        transformed_letter_map_index = np.linspace(0, 9, end, dtype='float64')\n",
    "    \n",
    "        transformed_alphabet_mapping = dict(zip(alphabet_data, transformed_letter_map_index))\n",
    "        word_letters = [letter for letter in self.word]\n",
    "            \n",
    "        return {\n",
    "            letter: transformed_alphabet_mapping[letter]\n",
    "            for letter in word_letters}\n",
    "        "
   ],
   "id": "7ae1b5c1df0d3bb7",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "[letter for letter in 'hello']",
   "id": "703de44f17ea10a9",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenized = TokenizedWord(word='what')\n",
    "\n",
    "tokenized.tokenized_word()"
   ],
   "id": "8b05c8ab2f3aaf32",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tokenize_word('hello')",
   "id": "ef062eee373b198d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def transform_data(data, r: tuple) -> np.ndarray:\n",
    "    \"\"\"Transform the `data` to fit range `r`, where `r=(rangeStart, rangeStop)`\"\"\"\n",
    "    min_orig = min(data)\n",
    "    max_orig = max(data)\n",
    "    normalized_data = [a + ((x - min_orig) * (r[1] - r[0]) / (max_orig - min_orig)) for x in data]\n",
    "    return np.array(normalized_data, dtype='float64')\n",
    "\n",
    "\n",
    "# Original data\n",
    "data = list(range(26))  # This generates a list [0, 1, 2, ..., 25]\n",
    "\n",
    "# Define the target range\n",
    "a = 0\n",
    "z = 1\n",
    "\n",
    "# Normalize the data\n",
    "normalized_data = transform_data(data, (a, z))\n",
    "print(normalized_data)"
   ],
   "id": "c9c049229c12835a",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "badac6786b231c34",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. User uploads an omex archive(fp) --> .\n",
    "# 2. Omex is unpacked into a temp dir\n",
    "# 3. report.h5 is extracted from temp dir (list of paths?)\n",
    "# 3a. expected-results.json extracted from temp dir\n",
    "# 4. Return dataclass from expected results json\n",
    "# 5. Extract species names/dataset labels by indexing over the datasets in expected results\n",
    "# 6. Use the output of #5 as an index over the report.h5 from #3.\n",
    "# 7. Extract the time points data from #4 and create/infer time vector.\n",
    "# 8. Use the inference from #7 as set data for both the Composite engine AND process itself (num steps).\n",
    "# 9. Verify the simulation settings in process constructor(s) against ExpectedResults (#5) BEFORE running \n",
    "# 10. Run the composite from #8 if #9 passes.\n",
    "# 11. Get the results from #10.\n",
    "# 12. Run Comparison matrices and assert all zeros."
   ],
   "id": "4b9f65536036a6f7",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T19:58:44.645234Z",
     "start_time": "2024-05-21T19:58:44.444828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from biosimulators_utils.combine.io import CombineArchiveReader\n",
    "from fastapi import FastAPI, UploadFile, File \n",
    "from fastapi.responses import StreamingResponse\n",
    "\n",
    "# app = FastAPI()\n",
    "\n",
    "\n",
    "class SourceServerExchange:\n",
    "    pass \n",
    "\n",
    "\n",
    "class UploadOmexExchange(SourceServerExchange):\n",
    "    app = FastAPI()\n",
    "    def __init__(self, app: FastAPI = app):\n",
    "        self.app = app \n",
    "        \n",
    "    @app.post(\"/upload-omex\")\n",
    "    async def upload_omex(self, file: UploadFile = File(...)):\n",
    "        contents = await file.read()\n",
    "        path = self.save_omex_archive(contents)\n",
    "        return {\"filename\": path}\n",
    "    \n",
    "    def save_omex_archive(self, contents: bytes) -> str:\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix='.omex') as temp_file:\n",
    "            temp_file.write(contents)\n",
    "            archive_path = temp_file.name\n",
    "        return archive_path\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "app = FastAPI()\n",
    "@app.post(\"/upload-omex\")\n",
    "async def upload_omex(file: UploadFile = File(...)):\n",
    "    contents = await file.read()\n",
    "    \n",
    "    save_dir = tempfile.mkdtemp()\n",
    "    archive_response = save_omex_archive(contents, save_dir)\n",
    "    return {\"filename\": archive_response['source']}\n",
    "\n",
    "\n",
    "def unpack_omex(archive_fp: str, save_dir: str):\n",
    "    return CombineArchiveReader().run(archive_fp, save_dir)\n",
    "\n",
    "\n",
    "def save_omex_archive(contents: bytes, save_dir: str):\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix='.omex') as temp_file:\n",
    "        temp_file.write(contents)\n",
    "        archive_path = temp_file.name\n",
    "    \n",
    "    return {'source': archive_path, 'archive': unpack_omex(archive_path, save_dir), 'save_dir': save_dir}"
   ],
   "id": "e15bceef13ab6cab",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T20:01:22.604852Z",
     "start_time": "2024-05-21T20:01:22.601504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from biosimulator_processes.services.rest_service import BiosimulationsRestService\n",
    "\n",
    "\n",
    "RUN_ID = '61fea4a08c1e3dc95a79802e'\n",
    "PROJECT_NAME = 'Repressilator'\n",
    "\n",
    "\n",
    "class VerifyResults:\n",
    "    def __init__(self, process_address_name: str, truth_run_id: str, truth_project_name: str):\n",
    "        self.process_address_name = process_address_name\n",
    "        self.truth_run_project_name = truth_project_name\n",
    "        self.biosim_service = BiosimulationsRestService()\n",
    "        \n",
    "        self.get_report_and_expected_results_from_project(truth_run_id, truth_project_name)\n",
    "    \n",
    "    def fetch_project_files(self, run_id: str, proj_name: str):\n",
    "        return self.biosim_service.get_project_files(run_id=run_id, project_name=proj_name)\n",
    "    \n",
    "    def get_report_and_expected_results_from_project(self, run_id, proj_name):\n",
    "        project_files = self.fetch_project_files(run_id, proj_name)\n",
    "        report_file = None \n",
    "        expected_results_file = None\n",
    "        \n",
    "        for file in project_files.files:\n",
    "            location = file['location']\n",
    "            if 'expected_results' in location:\n",
    "                expected_results_file = file \n",
    "            if 'report' in location:\n",
    "                report_file = file \n",
    "        \n",
    "        print(expected_results_file)"
   ],
   "id": "283df04da7d3e400",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-21T20:01:23.255668Z",
     "start_time": "2024-05-21T20:01:23.106727Z"
    }
   },
   "cell_type": "code",
   "source": "VerifyResults('copasi', RUN_ID, PROJECT_NAME)",
   "id": "713848772f8edba6",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "228f552cd30039b9",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
